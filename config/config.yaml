# Main Configuration File for Trump Prediction MVP

# Data Collection
data_collection:
  poll_interval_minutes: 5
  trump_user_id: "107780257626128497"
  historical_data_path: "data/raw/historical"
  
# Timing Model Configuration
timing_model:
  type: "prophet"  # Options: "prophet", "ntpp" (neural_tpp)

  # Prophet Configuration (traditional time-series)
  prophet:
    daily_seasonality: true
    weekly_seasonality: true
    yearly_seasonality: false
    changepoint_prior_scale: 0.05
    interval_width: 0.95

  # Neural Temporal Point Process Configuration (P0 implementation)
  # Better for high-frequency, bursty posting (20+ posts/day)
  neural_tpp:
    # Model architecture
    hidden_size: 64  # LSTM hidden dimension
    num_layers: 2  # Number of LSTM layers
    dropout: 0.1  # Dropout rate for regularization

    # Training parameters
    learning_rate: 0.001  # Adam learning rate
    epochs: 50  # Training epochs (50-100 recommended)
    batch_size: 32  # Batch size for training
    sequence_length: 20  # History length (number of previous posts)
    validation_split: 0.2  # Fraction for validation

    # Prediction parameters
    max_prediction_hours: 48  # Maximum hours to predict ahead
    sample_method: "expected"  # "expected" or "sample"

    # Device
    device: "auto"  # "auto", "cpu", or "cuda"

# Content Model Configuration
content_model:
  type: "claude_api"  # Options: claude_api, gpt2_finetuned, phi3_finetuned
  claude_api:
    model: "claude-sonnet-4-5-20250929"
    max_tokens: 280
    temperature: 0.8
    num_examples: 10  # Number of example posts in few-shot prompt
    max_calls_per_hour: 50  # Soft budget for Anthropic API usage
  
  # For future fine-tuned models
  finetuned:
    model_path: "models/finetuned"
    max_length: 280

# Feature Engineering
features:
  temporal:
    - hour
    - day_of_week
    - is_weekend
    - is_business_hours
    - time_since_last_post

  context:
    - recent_news
    - stock_market
    - trending_topics

# Advanced Feature Engineering Configuration (P2 Implementation)
feature_engineering:
  # Enable/disable feature groups
  enabled_features:
    temporal: true
    engagement: true
    historical: true
    context: true

  # Temporal features
  use_cyclical_encoding: true  # Use sin/cos encoding for periodic features

  # Engagement features
  engagement_windows: [5, 10, 20]  # Rolling window sizes
  normalize_engagement: true  # Normalize engagement metrics

  # Historical pattern features
  time_windows_hours: [1, 3, 6, 12, 24]  # Windows for counting posts
  lookback_periods: [5, 10, 20]  # Number of previous posts to analyze
  burst_threshold_minutes: 10  # Time threshold for burst detection

  # Context features
  market_threshold: 2.0  # Threshold for significant market moves (%)

  # Prophet regressors (features to use in Prophet model)
  prophet_regressors:
    # Temporal features (cyclical encoding captures periodicity better)
    - hour_sin
    - hour_cos
    - day_of_week_sin
    - day_of_week_cos
    - is_weekend
    - is_business_hours

    # Engagement features
    - engagement_total
    - engagement_rolling_mean_5
    - engagement_momentum

    # Historical pattern features (CRITICAL for burst detection)
    - time_since_last_hours
    - posts_last_1h
    - posts_last_3h
    - posts_last_6h
    - is_burst_post
    - burst_length
    - posting_regularity

    # Context features (reactive posting)
    - news_total_articles
    - is_breaking_news
    - is_trump_trending
    - market_avg_change
    - is_significant_market_move

# Evaluation Metrics
evaluation:
  timing_metrics:
    - mae
    - within_6h_accuracy
    - within_24h_accuracy
  
  content_metrics:
    - bertscore
    - bleu
    - perplexity

# Database
database:
  url: "sqlite:///./data/trump_predictions.db"
  pool_size: 5
  echo: false

# API Configuration
api:
  title: "Trump Post Prediction API"
  version: "0.1.0"
  description: "ML-powered prediction of Trump's next Truth Social post"
  host: "0.0.0.0"
  port: 8000

# Prediction Scheduling
scheduling:
  enabled: true  # Enable automatic predictions
  prediction_interval_hours: 6  # Make predictions every 6 hours
  # Alternative: cron expression (e.g., "0 */6 * * *" for every 6 hours)
  cron_expression: null  # If set, overrides prediction_interval_hours
  max_concurrent_predictions: 1  # Prevent overlapping jobs
  retry_on_failure: true
  retry_delay_minutes: 30

# Automated Model Retraining
retraining:
  enabled: true  # Enable automatic model retraining
  schedule_days: 7  # Retrain every N days (default: weekly)

  # Data split configuration
  test_split: 0.2  # Fraction of data for test set (0.2 = 20%)
  min_training_samples: 50  # Minimum samples required to train

  # Evaluation configuration
  evaluation_max_predictions: 20  # Max predictions to make during evaluation

  # Auto-promotion settings
  auto_promote: true  # Automatically promote better models
  min_improvement_threshold: 2.0  # Minimum % improvement to auto-promote (2% = 2.0)

  # Version management
  keep_versions: 10  # Keep N most recent versions per model type
  archive_old_versions: true  # Archive old versions to save space

  # Model-specific settings
  timing_model:
    enabled: true  # Enable timing model retraining
    evaluation_window_hours: 48  # Hours ahead to forecast during eval

  content_model:
    enabled: true  # Enable content model retraining
    update_examples: true  # Refresh example posts from latest data

# Prediction Validation
validation:
  enabled: true  # Enable automatic validation
  matching_window_hours: 24  # Time window for matching predictions to posts
  timing_threshold_hours: 6  # Within 6h = correct timing
  content_threshold: 0.3  # Minimum similarity for correct content

  # Enhanced Similarity Metrics Configuration
  similarity_metrics:
    # Enable/disable specific metrics
    use_bertscore: true  # BERTScore (semantic token-level matching)
    use_sentence_embeddings: true  # Sentence Transformers (document-level)
    use_entity_matching: true  # Named Entity Recognition matching

    # Metric weights (must sum to 1.0, will be normalized if not)
    weights:
      bertscore: 0.40  # 40% weight on BERTScore
      sentence: 0.30   # 30% weight on sentence embeddings
      entity: 0.20     # 20% weight on entity matching
      lexical: 0.10    # 10% weight on traditional lexical metrics

    # BERTScore model (options: roberta-large, microsoft/deberta-xlarge-mnli)
    bertscore_model: "microsoft/deberta-xlarge-mnli"

# Real-time Context Integration
context_integration:
  enabled: true  # Enable context fetching
  cache_ttl_minutes: 30  # Cache context for 30 minutes to avoid API limits
  save_to_database: true  # Save context snapshots to database

  # News API (NewsAPI.org)
  news_api_enabled: true
  news_max_articles: 10

  # Google Trends
  trends_enabled: true
  trends_num_topics: 10

  # Stock Market Data (yfinance)
  market_data_enabled: true

  # Per-source rate limits (minutes)
  rate_limits:
    news_minutes: 30
    trends_minutes: 60
    market_minutes: 15

  # RSS Feeds (fallback for news)
  rss_feeds_enabled: true
  rss_feeds:
    - https://rss.nytimes.com/services/xml/rss/nyt/Politics.xml
    - https://feeds.washingtonpost.com/rss/politics
    - https://www.politico.com/rss/politicopicks.xml

  # Future: Social media trends
  twitter_trends_enabled: false  # Requires Twitter API (not implemented yet)

  # Future: Political events calendar
  events_calendar_enabled: false  # Not implemented yet

# Dashboard
dashboard:
  title: "Trump Post Predictor"
  refresh_interval_seconds: 60
  show_confidence_intervals: true
  max_historical_predictions: 100

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "{time} | {level} | {message}"
  rotation: "500 MB"
  retention: "10 days"

# Monitoring & Alerts
monitoring:
  cron_alerts:
    max_consecutive_failures: 3
    window_minutes: 120
  accuracy_threshold:
    minimum_accuracy: 0.5  # Trigger alert if validation accuracy falls below this
  alert_channel: "log"  # For now, logs; can be slack/email later